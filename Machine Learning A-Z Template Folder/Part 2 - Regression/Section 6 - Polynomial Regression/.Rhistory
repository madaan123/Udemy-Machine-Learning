print(s)
setwd("C:/Users/DELL/Udemy-Machine-Learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 5 - Multiple Linear Regression")
dataset=read.csv('50_Startups.csv')
#dataset= dataset[, 2:3]
View(dataset)
View(dataset)
"installing a new library caTools to split our datasets into
training and testing sets ans include it by using library command
"
# install.packages('caTools')
library(caTools)
set.seed(123)
split=sample.split(dataset$Purchased,SplitRatio = 0.8)
training_set=subset(dataset,split==TRUE)
test_set=subset(dataset,split==FALSE)
library(caTools)
set.seed(123)
split=sample.split(dataset$Profit,SplitRatio = 0.8)
training_set=subset(dataset,split==TRUE)
test_set=subset(dataset,split==FALSE)
View(test_set)
View(training_set)
dataset$State=factor(dataset$State,
levels = c('New York','California','Florida'),
labels = c(1,2,3))
View(test_set)
View(dataset)
regressor= lm(formula= Profit ~ R.D.spend + Administration + Marketing.Spend + State,
data=training_set)
regressor= ln(formula= profit ~ . ,data=training_set)
regressor= lm(formula= profit ~ . ,data=training_set)
regressor= lm(formula= Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data=training_set)
regressor= lm(formula= Profit ~ . ,data=training_set)
regressor
regressor= lm(formula= Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data=training_set)
# we can also write the above statement as
# . symbolises profit depends on all independent variables
regressor= lm(formula= Profit ~ . ,data=training_set)
summary(regressor)
#Predicting our test set results
y_pred=predict(regressor, newdata = test_set)
y_pred
'''
Now we are going to implement backward elimination that considers only those columns or
independent variables that are statiscally more significant
we include all the columns and the remove the indignificant columns one by one
'''
"
we can check details of our regression by typing summary(regression on console)
it shows the number of stars higher the number more is the dependency of
dependent column on independent set and lesser the p value more is independent
variable statistically significant
R also handles the dummy trap and skips one dummy row
"
"
Now we are going to implement backward elimination that considers only those columns or
independent variables that are statiscally more significant
we include all the columns and the remove the indignificant columns one by one
"
regressor= lm(formula= Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data=dataset)
regressor= lm(formula= Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data=dataset)
summary(regressor)
regressor= lm(formula= Profit ~ R.D.Spend + Administration + Marketing.Spend ,
data=dataset)
regressor= lm(formula= Profit ~ R.D.Spend + Administration + Marketing.Spend ,
data=dataset)
summary(regressor)
regressor= lm(formula= Profit ~ R.D.Spend + Marketing.Spend ,
data=dataset)
summary(regressor)
regressor= lm(formula= Profit ~ R.D.Spend ,
data=dataset)
summary(regressor)
setwd("C:/Users/DELL/Udemy-Machine-Learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 6 - Polynomial Regression")
dataset=read.csv('Position_Salaries.csv')
View(dataset)
dataset= dataset[2:3]
View(dataset)
lin_reg = lm(Salary ~ . ,
data = dataset)
summary(lin_reg)
dataset$level2= dataset$Level^2    # create a new level that is square of level 1
dataset$level2= dataset$Level^3
dataset$level2= dataset$Level^4
dataset$level2= dataset$Level^2    # create a new level that is square of level 1
dataset$level3= dataset$Level^3
dataset$level4= dataset$Level^4
poly_reg = lm(formula = . ,
data = dataset)
poly_reg = lm(formula ~ . ,
data = dataset)
poly_reg = lm(Salary ~ . ,
data = dataset)
summary(poly_reg)
library(ggplot2)
library(ggplot2)
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour= 'red' ) +
geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)),
colour='blue') +
ggtitle('salary vs Experience(Linear Regression)') +
xlab('Years of experience') +
ylab('salary')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour= 'red' ) +
geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)),
colour='blue') +
ggtitle('Truth or Bluff(Linear Regression)') +
xlab('Years of experience') +
ylab('salary')
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour= 'red' ) +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)),
colour='blue') +
ggtitle('Truth or Bluff(Polynomial Regression)') +
xlab('Years of experience') +
ylab('salary')
View(test_set)
y_pred = predict(lin_reg,data.frame(level=6.5))
y_pred = predict(lin_reg,data.frame(Level=6.5))
y_pred
y_pred = predict(poly_reg,data.frame(Level=6.5))
y_pred = predict(poly_reg,data.frame(Level=6.5,
Level2=6.5^2),
Level3=6.5^3,
Level4=6.5^4)
y_pred = predict(poly_reg,data.frame(Level=6.5,
Level2=6.5^2,
Level3=6.5^3,
Level4=6.5^4)
y_pred
y_pred = predict(poly_reg,data.frame(Level=6.5,
Level2=6.5^2,
Level3=6.5^3,
Level4=6.5^4))
dataset$Level2= dataset$Level^2    # create a new level that is square of level 1
dataset$Level3= dataset$Level^3
dataset$Level4= dataset$Level^4
poly_reg = lm(Salary ~ . ,
data = dataset)
summary(poly_reg)
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour= 'red' ) +
geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)),
colour='blue') +
ggtitle('Truth or Bluff(Linear Regression)') +
xlab('Years of experience') +
ylab('salary')
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour= 'red' ) +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)),
colour='blue') +
ggtitle('Truth or Bluff(Polynomial Regression)') +
xlab('Years of experience') +
ylab('salary')
y_pred = predict(poly_reg,data.frame(Level=6.5,
Level2=6.5^2,
Level3=6.5^3,
Level4=6.5^4))
dataset=read.csv('Position_Salaries.csv')
dataset= dataset[2:3]
lin_reg = lm(Salary ~ . ,
data = dataset)
dataset$Level2= dataset$Level^2    # create a new level that is square of level 1
dataset$Level3= dataset$Level^3
dataset$Level4= dataset$Level^4
poly_reg = lm(Salary ~ . ,
data = dataset)
summary(poly_reg)
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour= 'red' ) +
geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)),
colour='blue') +
ggtitle('Truth or Bluff(Linear Regression)') +
xlab('Years of experience') +
ylab('salary')
"
y_pred = predict(lin_reg,data.frame(Level=6.5))
#Predicting a new value by polynomial regression
y_pred = predict(poly_reg,data.frame(Level=6.5,
Level2=6.5^2,
Level3=6.5^3,
Level4=6.5^4))
y_pred
y_pred = predict(poly_reg,data.frame(Level=6.5,
Level2=6.5^2,
Level3=6.5^3,
Level4=6.5^4))
y_pred
y_pred = predict(lin_reg,data.frame(Level=6.5))
y_pred = predict(poly_reg,data.frame(Level=6.5,
Level2=6.5^2,
Level3=6.5^3,
Level4=6.5^4))
View(dataset)
View(test_set)
View(training_set)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour= 'red' ) +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)),
colour='blue') +
ggtitle('Truth or Bluff(Polynomial Regression)') +
xlab('Years of experience') +
ylab('salary')
dataset=read.csv('Position_Salaries.csv')
dataset= dataset[2:3]
"installing a new library caTools to split our datasets into
training and testing sets ans include it by using library command
uncomment to split the dataset into test and training set
"
"
# install.packages('caTools')
library(caTools)
set.seed(123)
split=sample.split(dataset$Purchased,SplitRatio = 0.8)
training_set=subset(dataset,split==TRUE)
test_set=subset(dataset,split==FALSE)
"
"
machine learning models work on eucledian distances and we need to scale it as
it helps the algorithm to converge must faster
Now we are going to learn how to handle scaling in machine learning that
helps us to check whether large data entries do not overshadow
small valued entries
we have to include only those columns for scaling who have numeric values in dataset
"
# Feature Scaling
# training_set = scale(training_set)
# test_set = scale(test_set)
"
We are going to make both linear as well as polynomial regression and compare
both of them
"
#Fitting linear regression model to the dataset
lin_reg = lm(Salary ~ . ,
data = dataset)
"
we will modify our matrix will add new independent variables that will be just the
powers of the first independent variables poly_reg will be used to perform that task
"
#Fitting polynomial regression model to the dataset
dataset$Level2= dataset$Level^2    # create a new level that is square of level 1
dataset$Level3= dataset$Level^3
dataset$Level4= dataset$Level^4
poly_reg = lm(Salary ~ . ,
data = dataset)
summary(poly_reg)
dataset=read.csv('Position_Salaries.csv')
dataset= dataset[2:3]
lin_reg = lm(Salary ~ . ,
data = dataset)
dataset$Level2= dataset$Level^2    # create a new level that is square of level 1
dataset$Level3= dataset$Level^3
dataset$Level4= dataset$Level^4
poly_reg = lm(Salary ~ . ,
data = dataset)
summary(poly_reg)
summary(poly_reg)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour= 'red' ) +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)),
colour='blue') +
ggtitle('Truth or Bluff(Polynomial Regression)') +
xlab('Years of experience') +
ylab('salary')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour= 'red' ) +
geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)),
colour='blue') +
ggtitle('Truth or Bluff(Linear Regression)') +
xlab('Years of experience') +
ylab('salary')
y_pred = predict(lin_reg,data.frame(Level=6.5))
#Predicting a new value by polynomial regression
y_pred = predict(poly_reg,data.frame(Level=6.5,
Level2=6.5^2,
Level3=6.5^3,
Level4=6.5^4))
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour= 'red' ) +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)),
colour='blue') +
ggtitle('Truth or Bluff(Polynomial Regression)') +
xlab('Years of experience') +
ylab('salary')
dataset=read.csv('Position_Salaries.csv')
dataset= dataset[2:3]
lin_reg = lm(Salary ~ . ,
data = dataset)
summary(lin_reg)
